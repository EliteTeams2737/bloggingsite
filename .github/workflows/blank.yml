name: Scrape and Process Website

on:
  schedule:
    - cron: '*/5 * * * *' # Runs every 5 minutes
  workflow_dispatch: # Manual trigger

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install beautifulsoup4 requests selenium

    - name: Install Chrome and ChromeDriver
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser
        CHROME_VERSION=$(google-chrome --version | grep -oP '[0-9.]+')
        CHROMEDRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION)
        wget https://chromedriver.storage.googleapis.com/$CHROMEDRIVER_VERSION/chromedriver_linux64.zip
        unzip chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver

    - name: Download Website Content with Selenium
      run: |
        echo 'from selenium import webdriver
import time
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument("--headless")  # Run in headless mode
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
service = Service("/usr/local/bin/chromedriver")

driver = webdriver.Chrome(service=service, options=options)
driver.get("https://educationwithwork.wixsite.com/index")

# Scroll to the bottom of the page to load all content
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)  # Wait for new content to load
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# Save the page source to a file
with open("site/content.html", "w", encoding="utf-8") as f:
    f.write(driver.page_source)

driver.quit()' > scrape.py

        python scrape.py

    - name: Run Python Processing Script
      run: python process.py  # Assuming your script is named process.py and is located in the repo

    - name: Copy Files to Main Directory
      run: python process.py  # process.py will also handle copying to the main directory

    - name: Pull latest changes from GitHub
      run: git pull origin main  # Pull the latest changes before pushing

    - name: Commit changes
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git add .
        git commit -m "Processed and scraped website"
    
    - name: Push changes
      run: |
        git push 
